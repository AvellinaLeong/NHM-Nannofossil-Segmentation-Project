{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnGv6k/m22IfNlZa42aOCi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvellinaLeong/NHM-Nannofossil-Segmentation-Project/blob/main/03_evaluation_val_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Set script location to own development space\n",
        "MY_DEVELOPMENT_SPACE = '/content/drive/MyDrive/development/avellina/'\n",
        "import os\n",
        "os.chdir(MY_DEVELOPMENT_SPACE)\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CibX5UiAJRo",
        "outputId": "dbb9ddf6-ae85-45d8-9b65-536835f50758"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/development/avellina\n",
            "detectron2  Detectron2_notebooks  Mask-RCNN  Morphometrics_notebooks  output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find Precision, Recall and Accuracy"
      ],
      "metadata": {
        "id": "wRIWKzNx9bDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install pyyaml==5.1\n",
        "import sys, os, distutils.core\n",
        "!git clone 'https://github.com/facebookresearch/detectron2'\n",
        "dist = distutils.core.run_setup(\"./detectron2/setup.py\")\n",
        "!python -m pip install {' '.join([f\"'{x}'\" for x in dist.install_requires])}\n",
        "sys.path.insert(0, os.path.abspath('./detectron2'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jufmc1Yz_7fr",
        "outputId": "cb4c1d89-7e82-4920-f7c7-aa8d2a551bcb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyyaml==5.1\n",
            "  Downloading PyYAML-5.1.tar.gz (274 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/274.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.2/274.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "fatal: destination path 'detectron2' already exists and is not an empty directory.\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (2.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Collecting yacs>=0.1.8\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Collecting fvcore<0.1.6,>=0.1.5\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting iopath<0.1.10,>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting omegaconf<2.4,>=2.1\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black\n",
            "  Downloading black-24.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (24.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.25.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from yacs>=0.1.8) (6.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Collecting portalocker (from iopath<0.1.10,>=0.1.7)\n",
            "  Downloading portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4,>=2.1)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from black) (8.1.7)\n",
            "Collecting mypy-extensions>=0.4.3 (from black)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting pathspec>=0.9.0 (from black)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.10/dist-packages (from black) (4.2.2)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from black) (2.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from black) (4.12.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Building wheels for collected packages: fvcore, antlr4-python3-runtime\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=bfa516b4e404e7a68877dda864659638efd3f1f649938d8476637c0426579a88\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=e05ad7529765a1aede4f2c44edfbb2ad1dca90bc170bb2e0b842da3195a3da8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built fvcore antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, yacs, portalocker, pathspec, omegaconf, mypy-extensions, iopath, hydra-core, black, fvcore\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-24.4.2 fvcore-0.1.5.post20221221 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 pathspec-0.12.1 portalocker-2.10.0 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, detectron2\n",
        "!nvcc --version\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "print(\"detectron2:\", detectron2.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Kov2BvLjAPnR",
        "outputId": "60c2ff72-4fb7-455e-8bba-634afb3e66bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "torch:  2.3 ; cuda:  cu121\n",
            "detectron2: 0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Detectron2 and Detectron2 Logger\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdFtYtSZASBc",
        "outputId": "4c7bedb9-8d29-406e-afde-58165cd273d0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Logger detectron2 (DEBUG)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import copy\n",
        "import numpy as np\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data.datasets import register_coco_instances\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader, transforms as T\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.data import detection_utils as utils\n",
        "from detectron2.engine import DefaultTrainer\n",
        "import cv2\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode"
      ],
      "metadata": {
        "id": "SMqQbA5qAUtF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Register datasets\n",
        "register_coco_instances(\"my_dataset_val\", {}, \"/content/drive/MyDrive/data/species_53/data/val/coco_val.json\", \"/content/drive/MyDrive/data/species_53/data/val\")\n",
        "register_coco_instances(\"my_dataset_test\", {}, \"/content/drive/MyDrive/data/species_53/data/test/coco_test.json\", \"/content/drive/MyDrive/data/species_53/data/test\")"
      ],
      "metadata": {
        "id": "ewkpe-6gAZ8c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load metadata for visualization\n",
        "val_metadata = MetadataCatalog.get(\"my_dataset_val\")\n",
        "test_metadata = MetadataCatalog.get(\"my_dataset_test\")\n",
        "\n",
        "# Get validation and test dataset dicts\n",
        "val_dataset_dicts = DatasetCatalog.get(\"my_dataset_val\")\n",
        "test_dataset_dicts = DatasetCatalog.get(\"my_dataset_test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoujaSpfkIFL",
        "outputId": "7914dd07-6cd7-43be-ade5-ae8a2e8fc8c0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 15:57:34 d2.data.datasets.coco]: Loaded 95 images in COCO format from /content/drive/MyDrive/data/species_53/data/val/coco_val.json\n",
            "[06/28 15:57:34 d2.data.datasets.coco]: Loaded 96 images in COCO format from /content/drive/MyDrive/data/species_53/data/test/coco_test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Augmentations and Load Configurations"
      ],
      "metadata": {
        "id": "FV6lJqAtAuLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define augmentations\n",
        "from detectron2.data import transforms as T\n",
        "\n",
        "augs = T.AugmentationList([\n",
        "    T.RandomBrightness(0.9, 1.1),\n",
        "    T.RandomFlip(prob=0.5),\n",
        "    T.RandomRotation(angle=[-90, 90]),\n",
        "    T.RandomSaturation(0.8, 1.2),\n",
        "])"
      ],
      "metadata": {
        "id": "MPkodnWHAenK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom data mapper\n",
        "def custom_mapper(dataset_dict):\n",
        "    dataset_dict = copy.deepcopy(dataset_dict)\n",
        "    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "\n",
        "    # Get the annotations\n",
        "    annos = dataset_dict.get(\"annotations\", [])\n",
        "    bbox_list = [BoxMode.convert(obj[\"bbox\"], obj[\"bbox_mode\"], BoxMode.XYXY_ABS) for obj in annos]\n",
        "    bbox_list = np.array(bbox_list, dtype=np.float32)\n",
        "\n",
        "    # Apply augmentations\n",
        "    aug_input = T.AugInput(image, boxes=bbox_list)\n",
        "    transforms = augs(aug_input)\n",
        "    image = aug_input.image\n",
        "\n",
        "    # Apply the same transforms to the annotations\n",
        "    annos = [utils.transform_instance_annotations(obj, transforms, image.shape[:2]) for obj in annos]\n",
        "    instances = utils.annotations_to_instances(annos, image.shape[:2])\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n",
        "    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "id": "pM46yMRt7GOj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = \"/content/drive/MyDrive/data/species_53/Detectron2_Models/1\"\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
        "cfg.DATASETS.VAL = (\"my_dataset_val\",)\n",
        "cfg.DATASETS.TEST = (\"my_dataset_test\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final_1.pth\")  # Make sure this path is correct\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025\n",
        "cfg.SOLVER.MAX_ITER = 1000   #iterations -- can change\n",
        "cfg.SOLVER.STEPS = []\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512 # Default is 512\n",
        "cfg.TEST.EVAL_PERIOD = 500  # Evaluate every 500 iterations\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # Number of classes\n",
        "\n",
        "# Load the saved configuration from the YAML file\n",
        "config_yaml_path = \"/content/drive/MyDrive/data/species_53/Detectron2_Models/1/config_1.yaml\"\n",
        "cfg.merge_from_file(config_yaml_path)\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"1\", \"model_final_1.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.50"
      ],
      "metadata": {
        "id": "1H0LbYWQ7IyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c9b8ae-5643-42af-8ad7-4cabafa24ae7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:fvcore.common.config:Loading config /content/drive/MyDrive/data/species_53/Detectron2_Models/1/config_1.yaml with yaml.unsafe_load. Your machine may be at risk if the file contains malicious content.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the predictor\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk7Tj8BCBk43",
        "outputId": "b03b884a-9476-4541-d628-386e11c36208"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 14:58:33 d2.checkpoint.detection_checkpoint]: [DetectionCheckpointer] Loading from /content/drive/MyDrive/data/species_53/Detectron2_Models/1/model_final_1.pth ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision and Recall on Validation"
      ],
      "metadata": {
        "id": "Tmnl5H4QBlt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and Recall on validation\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.data import build_detection_test_loader\n",
        "evaluator = COCOEvaluator(\"my_dataset_val\", output_dir=\"./output\")\n",
        "val_loader = build_detection_test_loader(cfg, \"my_dataset_val\")\n",
        "\n",
        "results = inference_on_dataset(predictor.model, val_loader, evaluator)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2_F7DCjWBoZX",
        "outputId": "4626e567-701d-4986-af05-bb0d3b3ce607"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 14:58:44 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[06/28 14:58:45 d2.data.datasets.coco]: Loaded 95 images in COCO format from /content/drive/MyDrive/data/species_53/data/val/coco_val.json\n",
            "[06/28 14:58:45 d2.data.build]: Distribution of instances among all 1 categories:\n",
            "|  category   | #instances   |\n",
            "|:-----------:|:-------------|\n",
            "| t_orionatus | 103          |\n",
            "|             |              |\n",
            "[06/28 14:58:45 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[06/28 14:58:45 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[06/28 14:58:45 d2.data.common]: Serializing 95 elements to byte tensors and concatenating them all ...\n",
            "[06/28 14:58:45 d2.data.common]: Serialized dataset takes 0.03 MiB\n",
            "[06/28 14:58:45 d2.evaluation.evaluator]: Start inference on 95 batches\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 14:58:52 d2.evaluation.evaluator]: Inference done 11/95. Dataloading: 0.2374 s/iter. Inference: 0.0727 s/iter. Eval: 0.0003 s/iter. Total: 0.3104 s/iter. ETA=0:00:26\n",
            "[06/28 14:58:57 d2.evaluation.evaluator]: Inference done 23/95. Dataloading: 0.3139 s/iter. Inference: 0.0731 s/iter. Eval: 0.0004 s/iter. Total: 0.3875 s/iter. ETA=0:00:27\n",
            "[06/28 14:59:02 d2.evaluation.evaluator]: Inference done 35/95. Dataloading: 0.3226 s/iter. Inference: 0.0782 s/iter. Eval: 0.0004 s/iter. Total: 0.4013 s/iter. ETA=0:00:24\n",
            "[06/28 14:59:07 d2.evaluation.evaluator]: Inference done 48/95. Dataloading: 0.3219 s/iter. Inference: 0.0835 s/iter. Eval: 0.0004 s/iter. Total: 0.4060 s/iter. ETA=0:00:19\n",
            "[06/28 14:59:13 d2.evaluation.evaluator]: Inference done 60/95. Dataloading: 0.3294 s/iter. Inference: 0.0840 s/iter. Eval: 0.0004 s/iter. Total: 0.4139 s/iter. ETA=0:00:14\n",
            "[06/28 14:59:18 d2.evaluation.evaluator]: Inference done 73/95. Dataloading: 0.3300 s/iter. Inference: 0.0836 s/iter. Eval: 0.0004 s/iter. Total: 0.4141 s/iter. ETA=0:00:09\n",
            "[06/28 14:59:23 d2.evaluation.evaluator]: Inference done 86/95. Dataloading: 0.3277 s/iter. Inference: 0.0856 s/iter. Eval: 0.0004 s/iter. Total: 0.4139 s/iter. ETA=0:00:03\n",
            "[06/28 14:59:27 d2.evaluation.evaluator]: Total inference time: 0:00:37.332741 (0.414808 s / iter per device, on 1 devices)\n",
            "[06/28 14:59:27 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:07 (0.084844 s / iter per device, on 1 devices)\n",
            "[06/28 14:59:28 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[06/28 14:59:28 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[06/28 14:59:29 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.190\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.094\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.198\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.217\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.250\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[06/28 14:59:29 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
            "| 19.038 | 41.225 | 9.397  |  nan  | 19.848 | 0.000 |\n",
            "[06/28 14:59:29 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.03s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.412\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.276\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.302\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.345\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.345\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[06/28 14:59:29 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
            "| 26.342 | 41.225 | 27.562 |  nan  | 27.397 | 0.000 |\n",
            "[06/28 14:59:29 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "OrderedDict([('bbox', {'AP': 19.037999713543115, 'AP50': 41.22544619272258, 'AP75': 9.396827958079847, 'APs': nan, 'APm': 19.848004900345682, 'APl': 0.0}), ('segm', {'AP': 26.342023553406342, 'AP50': 41.22544619272258, 'AP75': 27.562465281545528, 'APs': nan, 'APm': 27.39668453378413, 'APl': 0.0})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision and Recall on Validation Set Results\n",
        "\n",
        "# Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.263\n",
        "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.412\n",
        "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.276\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.274\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.302\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.345\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.345\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.351\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000"
      ],
      "metadata": {
        "id": "fghDXbERaFN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to csv (but only precision gets saved need to debug this later)\n",
        "import csv\n",
        "import os\n",
        "\n",
        "# Save evaluation as a csv file in folder \"Detectron2_Models\"\n",
        "evaluation_csv_file_path = \"/content/drive/MyDrive/data/species_53/Detectron2_Models/evaluation/recall_precision_val.csv\"\n",
        "os.makedirs(os.path.dirname(evaluation_csv_file_path), exist_ok=True)\n",
        "\n",
        "# Extract metrics\n",
        "metrics = results['segm']  # 'segm' for segmentation mask, 'bbox' for bounding box\n",
        "\n",
        "# Debug: Print the metrics dictionary to ensure it contains the expected keys\n",
        "print(\"Metrics dictionary:\", metrics)\n",
        "\n",
        "#CSV header\n",
        "header = [\n",
        "    'Metric', 'AP', 'AP50', 'AP75', 'APs', 'APm', 'APl',\n",
        "    'AR@1', 'AR@10', 'AR@100', 'ARs', 'ARm', 'ARl'\n",
        "]\n",
        "\n",
        "# Open CSV\n",
        "with open(evaluation_csv_file_path, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # Prepare data row\n",
        "    data_row = [\n",
        "        'Segmentation Mask',\n",
        "        metrics.get('AP', ''), metrics.get('AP50', ''), metrics.get('AP75', ''), metrics.get('APs', ''), metrics.get('APm', ''), metrics.get('APl', ''),\n",
        "        metrics.get('AR@1', ''), metrics.get('AR@10', ''), metrics.get('AR@100', ''), metrics.get('ARs', ''), metrics.get('ARm', ''), metrics.get('ARl', '')\n",
        "    ]\n",
        "\n",
        "    # Debug: Print the data row to verify its content before writing\n",
        "    print(\"Data row:\", data_row)\n",
        "\n",
        "    writer.writerow(data_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QdROMA5eBu91",
        "outputId": "3c936fad-30e4-4487-e478-62e041ed2752"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics dictionary: {'AP': 26.342023553406342, 'AP50': 41.22544619272258, 'AP75': 27.562465281545528, 'APs': nan, 'APm': 27.39668453378413, 'APl': 0.0}\n",
            "Data row: ['Segmentation Mask', 26.342023553406342, 41.22544619272258, 27.562465281545528, nan, 27.39668453378413, 0.0, '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Precision and Recall on Test"
      ],
      "metadata": {
        "id": "DqZr3-1NTlQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy and Recall on test\n",
        "evaluator = COCOEvaluator(\"my_dataset_test\", output_dir=\"./output\")\n",
        "test_loader = build_detection_test_loader(cfg, \"my_dataset_test\")\n",
        "results = inference_on_dataset(predictor.model, test_loader, evaluator)\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jn0g0BghTnj0",
        "outputId": "c66e5457-599a-4845-bad9-9869c9e1fb5c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 15:15:27 d2.evaluation.coco_evaluation]: Fast COCO eval is not built. Falling back to official COCO eval.\n",
            "[06/28 15:15:29 d2.data.datasets.coco]: Loaded 96 images in COCO format from /content/drive/MyDrive/data/species_53/data/test/coco_test.json\n",
            "[06/28 15:15:29 d2.data.build]: Distribution of instances among all 1 categories:\n",
            "|  category   | #instances   |\n",
            "|:-----------:|:-------------|\n",
            "| t_orionatus | 108          |\n",
            "|             |              |\n",
            "[06/28 15:15:29 d2.data.dataset_mapper]: [DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "[06/28 15:15:29 d2.data.common]: Serializing the dataset using: <class 'detectron2.data.common._TorchSerializedList'>\n",
            "[06/28 15:15:29 d2.data.common]: Serializing 96 elements to byte tensors and concatenating them all ...\n",
            "[06/28 15:15:29 d2.data.common]: Serialized dataset takes 0.03 MiB\n",
            "[06/28 15:15:29 d2.evaluation.evaluator]: Start inference on 96 batches\n",
            "[06/28 15:15:34 d2.evaluation.evaluator]: Inference done 11/96. Dataloading: 0.3404 s/iter. Inference: 0.0716 s/iter. Eval: 0.0004 s/iter. Total: 0.4124 s/iter. ETA=0:00:35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/drive/MyDrive/development/avellina/detectron2/detectron2/layers/wrappers.py:142: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  x = F.conv2d(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 15:15:39 d2.evaluation.evaluator]: Inference done 23/96. Dataloading: 0.3048 s/iter. Inference: 0.1121 s/iter. Eval: 0.0005 s/iter. Total: 0.4175 s/iter. ETA=0:00:30\n",
            "[06/28 15:15:44 d2.evaluation.evaluator]: Inference done 37/96. Dataloading: 0.3171 s/iter. Inference: 0.0956 s/iter. Eval: 0.0004 s/iter. Total: 0.4133 s/iter. ETA=0:00:24\n",
            "[06/28 15:15:50 d2.evaluation.evaluator]: Inference done 51/96. Dataloading: 0.3210 s/iter. Inference: 0.0897 s/iter. Eval: 0.0004 s/iter. Total: 0.4113 s/iter. ETA=0:00:18\n",
            "[06/28 15:15:56 d2.evaluation.evaluator]: Inference done 65/96. Dataloading: 0.3211 s/iter. Inference: 0.0862 s/iter. Eval: 0.0004 s/iter. Total: 0.4079 s/iter. ETA=0:00:12\n",
            "[06/28 15:16:01 d2.evaluation.evaluator]: Inference done 77/96. Dataloading: 0.3275 s/iter. Inference: 0.0844 s/iter. Eval: 0.0004 s/iter. Total: 0.4124 s/iter. ETA=0:00:07\n",
            "[06/28 15:16:06 d2.evaluation.evaluator]: Inference done 91/96. Dataloading: 0.3246 s/iter. Inference: 0.0835 s/iter. Eval: 0.0004 s/iter. Total: 0.4087 s/iter. ETA=0:00:02\n",
            "[06/28 15:16:08 d2.evaluation.evaluator]: Total inference time: 0:00:36.899999 (0.405494 s / iter per device, on 1 devices)\n",
            "[06/28 15:16:08 d2.evaluation.evaluator]: Total inference pure compute time: 0:00:07 (0.083044 s / iter per device, on 1 devices)\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Preparing results for COCO format ...\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Saving results to ./output/coco_instances_results.json\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Evaluating predictions with official COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *bbox*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.124\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.295\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.049\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.128\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.157\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.184\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.188\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Evaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
            "| 12.433 | 29.511 | 4.891  |  nan  | 12.815 | 0.000 |\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "Running per image evaluation...\n",
            "Evaluate annotation type *segm*\n",
            "DONE (t=0.02s).\n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.01s).\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.201\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.281\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.251\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.282\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Evaluation results for segm: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl  |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:-----:|\n",
            "| 20.110 | 28.067 | 22.535 |  nan  | 22.152 | 0.000 |\n",
            "[06/28 15:16:08 d2.evaluation.coco_evaluation]: Some metrics cannot be computed and is shown as NaN.\n",
            "OrderedDict([('bbox', {'AP': 12.43302989470257, 'AP50': 29.511116126101783, 'AP75': 4.890935902100848, 'APs': nan, 'APm': 12.815397127246023, 'APl': 0.0}), ('segm', {'AP': 20.11010074019501, 'AP50': 28.067482710285717, 'AP75': 22.535429716817312, 'APs': nan, 'APm': 22.152352982125365, 'APl': 0.0})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision and Recall on Test Set Results\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.201\n",
        "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.281\n",
        "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.225\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.222\n",
        "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.251\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.282\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.282\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.288\n",
        "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000"
      ],
      "metadata": {
        "id": "p1LdCvgAa2kg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save evaluation as a CSV file\n",
        "evaluation_csv_file_path = \"/content/drive/MyDrive/data/species_53/Detectron2_Models/evaluation/recall_precision_test.csv\"\n",
        "os.makedirs(os.path.dirname(evaluation_csv_file_path), exist_ok=True)\n",
        "\n",
        "# Extract relevant metrics from results['segm']\n",
        "metrics = results['segm']  # Assuming 'segm' contains segmentation mask metrics\n",
        "\n",
        "#CSV header\n",
        "header = [\n",
        "    'Metric', 'AP', 'AP50', 'AP75', 'APs', 'APm', 'APl',\n",
        "    'AR@1', 'AR@10', 'AR@100', 'ARs', 'ARm', 'ARl'\n",
        "]\n",
        "\n",
        "# Open CSV\n",
        "with open(evaluation_csv_file_path, 'w', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "\n",
        "    # Prepare data row\n",
        "    data_row = [\n",
        "        'Segmentation Mask',\n",
        "        metrics.get('AP', ''), metrics.get('AP50', ''), metrics.get('AP75', ''), metrics.get('APs', ''), metrics.get('APm', ''), metrics.get('APl', ''),\n",
        "        metrics.get('AR@1', ''), metrics.get('AR@10', ''), metrics.get('AR@100', ''), metrics.get('ARs', ''), metrics.get('ARm', ''), metrics.get('ARl', '')\n",
        "    ]\n",
        "\n",
        "    # Debug: Print the data row to verify its content before writing\n",
        "    print(\"Data row:\", data_row)\n",
        "\n",
        "    writer.writerow(data_row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfvD08aUTnwV",
        "outputId": "2ca320d1-e627-41e0-8d23-57209c6481c4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data row: ['Segmentation Mask', 20.11010074019501, 28.067482710285717, 22.535429716817312, nan, 22.152352982125365, 0.0, '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy on Validation and Test"
      ],
      "metadata": {
        "id": "uOOHfMPTToD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shapely rasterio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3rChMka3cXGn",
        "outputId": "4612dacf-049a-4260-daca-13a7591e9f6d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (2.0.4)\n",
            "Collecting rasterio\n",
            "  Downloading rasterio-1.3.10-cp310-cp310-manylinux2014_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely) (1.25.2)\n",
            "Collecting affine (from rasterio)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from rasterio) (23.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from rasterio) (2024.6.2)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.10/dist-packages (from rasterio) (8.1.7)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.10/dist-packages (from rasterio) (0.7.2)\n",
            "Collecting snuggs>=1.4.1 (from rasterio)\n",
            "  Downloading snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.10/dist-packages (from rasterio) (1.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from rasterio) (67.7.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.10/dist-packages (from snuggs>=1.4.1->rasterio) (3.1.2)\n",
            "Installing collected packages: snuggs, affine, rasterio\n",
            "Successfully installed affine-2.4.0 rasterio-1.3.10 snuggs-1.4.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from shapely.geometry import Polygon\n",
        "import rasterio.features\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "def accuracy(groundtruth_mask, pred_mask):\n",
        "    intersect = np.sum(pred_mask * groundtruth_mask)\n",
        "    union = np.sum(pred_mask) + np.sum(groundtruth_mask) - intersect\n",
        "    xor = np.sum(groundtruth_mask == pred_mask)\n",
        "    acc = np.mean(xor / (union + xor - intersect))\n",
        "    return round(acc, 3)\n",
        "\n",
        "def convert_polygons_to_mask(polygons, height, width):\n",
        "    mask = np.zeros((height, width), dtype=np.uint8)\n",
        "\n",
        "    try:\n",
        "        for polygon in polygons:\n",
        "            # Create a Shapely polygon object from the list of points\n",
        "            poly = Polygon(polygon)\n",
        "\n",
        "            # Rasterize the polygon into the mask\n",
        "            if poly.is_valid:  # Check if the polygon is valid\n",
        "                try:\n",
        "                    # Use rasterio.features to rasterize the polygon\n",
        "                    mask_shape = [(poly, 1)]\n",
        "                    mask = rasterio.features.geometry_mask(mask_shape, out_shape=(height, width))\n",
        "\n",
        "                    # Ensure mask is boolean (optional)\n",
        "                    mask = mask.astype(np.uint8)  # Convert to uint8 (0 or 1)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error rasterizing polygon: {e}\")\n",
        "                    continue\n",
        "            else:\n",
        "                print(\"Invalid polygon detected, skipping...\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error converting polygons to mask: {e}\")\n",
        "\n",
        "    return mask\n",
        "\n",
        "def compute_accuracy_on_dataset(dataset_name):\n",
        "    dataset_dicts = DatasetCatalog.get(dataset_name)\n",
        "    total_accuracy = 0\n",
        "    count = 0\n",
        "\n",
        "    for dataset_dict in dataset_dicts:\n",
        "        image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n",
        "        outputs = predictor(image)\n",
        "        pred_masks = outputs[\"instances\"].pred_masks.cpu().numpy()\n",
        "\n",
        "        if \"annotations\" in dataset_dict:\n",
        "            annos = dataset_dict[\"annotations\"]\n",
        "            gt_masks = []\n",
        "            for anno in annos:\n",
        "                if 'segmentation' in anno:\n",
        "                    masks = convert_polygons_to_mask(anno[\"segmentation\"], image.shape[0], image.shape[1])\n",
        "                    gt_masks.append(masks)\n",
        "            gt_masks = np.array(gt_masks)\n",
        "\n",
        "            if len(gt_masks) > 0 and len(pred_masks) > 0:\n",
        "                gt_mask_combined = np.max(gt_masks, axis=0)\n",
        "                pred_mask_combined = np.max(pred_masks, axis=0)\n",
        "                acc = accuracy(gt_mask_combined, pred_mask_combined)\n",
        "                total_accuracy += acc\n",
        "                count += 1\n",
        "\n",
        "    return total_accuracy / count if count > 0 else 0\n",
        "\n",
        "# Calculate accuracy on both test and val datasets\n",
        "val_accuracy = compute_accuracy_on_dataset(\"my_dataset_val\")\n",
        "test_accuracy = compute_accuracy_on_dataset(\"my_dataset_test\")\n",
        "\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6N5ABR3hc-C2",
        "outputId": "1249b3c1-efca-4629-fa4b-c8eb085eb58e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/28 15:57:36 d2.data.datasets.coco]: Loaded 95 images in COCO format from /content/drive/MyDrive/data/species_53/data/val/coco_val.json\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "[06/28 15:57:46 d2.data.datasets.coco]: Loaded 96 images in COCO format from /content/drive/MyDrive/data/species_53/data/test/coco_test.json\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Error converting polygons to mask: 'int' object is not iterable\n",
            "Validation Accuracy: 0.9666799999999998\n",
            "Test Accuracy: 0.966127659574468\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Accuracy: 0.9666799999999998\n",
        "Test Accuracy: 0.966127659574468"
      ],
      "metadata": {
        "id": "6xDhXwzLgGJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IOU on Validation and Test\n",
        "\n",
        "\n",
        "\n",
        "*   Each IOU\n",
        "*   Average IOU\n",
        "\n"
      ],
      "metadata": {
        "id": "V0Maq0Ach-rD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get segmentation mask predictions for val and test datasets\n",
        "val_predictions = []\n",
        "test_predictions = []\n",
        "\n",
        "for d in val_dataset_dicts:\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    val_predictions.append(outputs[\"instances\"].to(\"cpu\").get(\"pred_masks\"))\n",
        "\n",
        "for d in test_dataset_dicts:\n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)\n",
        "    test_predictions.append(outputs[\"instances\"].to(\"cpu\").get(\"pred_masks\"))"
      ],
      "metadata": {
        "id": "R_rEvj2DlEes"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(predicted_masks, gt_masks):\n",
        "    \"\"\"\n",
        "    Calculate IoU between predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        predicted_masks (list): List of predicted segmentation masks.\n",
        "        gt_masks (list): List of ground truth segmentation masks.\n",
        "    Returns:\n",
        "        list: List of IoU values.\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "    for pred_mask, gt_mask in zip(predicted_masks, gt_masks):\n",
        "        iou = compute_iou(pred_mask, gt_mask)\n",
        "        ious.append(iou)\n",
        "\n",
        "    return ious\n",
        "\n",
        "def compute_iou(pred_mask, gt_mask):\n",
        "    \"\"\"\n",
        "    Compute IoU between predicted mask and ground truth mask.\n",
        "    Args:\n",
        "        pred_mask (numpy.ndarray): Predicted segmentation mask (binary).\n",
        "        gt_mask (numpy.ndarray): Ground truth segmentation mask (binary).\n",
        "    Returns:\n",
        "        float: IoU value.\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "    iou = intersection / union if union > 0 else 0.0\n",
        "    return iou\n",
        "\n",
        "# Example usage to calculate IoU for validation and test sets\n",
        "val_gt_masks = [cv2.imread(d[\"file_name\"])[:, :, 0] for d in val_dataset_dicts]  # Assuming ground truth masks are stored as images\n",
        "test_gt_masks = [cv2.imread(d[\"file_name\"])[:, :, 0] for d in test_dataset_dicts]  # Assuming ground truth masks are stored as images\n",
        "\n",
        "val_ious = calculate_iou(val_predictions, val_gt_masks)\n",
        "test_ious = calculate_iou(test_predictions, test_gt_masks)\n",
        "\n",
        "# Print or process the IoU values as needed\n",
        "print(\"IoU for validation set:\")\n",
        "print(val_ious)\n",
        "\n",
        "print(\"IoU for test set:\")\n",
        "print(test_ious)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eMiO3uVlFJS",
        "outputId": "fa7baa28-79d6-461b-dd44-e82beb627c40"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IoU for validation set:\n",
            "[0.0, 0.0, tensor(0.0381), tensor(0.0298), tensor(0.0354), tensor(0.0474), 0.0, tensor(0.0381), 0.0, 0.0, 0.0, tensor(0.0177), tensor(0.0085), tensor(0.0054), tensor(0.0199), 0.0, tensor(0.0324), tensor(0.0301), tensor(0.0451), 0.0, 0.0, 0.0, tensor(0.0213), 0.0, 0.0, 0.0, 0.0, tensor(0.0432), tensor(0.0309), tensor(0.0202), tensor(0.0309), 0.0, tensor(0.0105), 0.0, tensor(0.0380), tensor(0.0223), tensor(0.0254), 0.0, tensor(0.0443), tensor(0.0469), 0.0, tensor(0.0492), tensor(0.0193), 0.0, tensor(0.0253), 0.0, 0.0, tensor(0.0385), tensor(0.0276), tensor(0.0415), 0.0, 0.0, 0.0, 0.0, tensor(0.0153), tensor(0.0313), tensor(0.0319), 0.0, 0.0, 0.0, tensor(0.0437), tensor(0.0271), tensor(0.0316), tensor(0.0340), 0.0, 0.0, tensor(0.0137), tensor(0.0318), tensor(0.0358), 0.0, 0.0, 0.0, 0.0, 0.0, tensor(0.0196), tensor(0.0290), tensor(0.0229), tensor(0.0339), 0.0, tensor(0.0365), tensor(0.0231), 0.0, tensor(0.0296), tensor(0.0295), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, tensor(0.0167), 0.0, tensor(0.0365), 0.0, tensor(0.0337)]\n",
            "IoU for test set:\n",
            "[0.0, tensor(0.0284), tensor(0.0066), 0.0, 0.0, 0.0, 0.0, tensor(0.0126), 0.0, tensor(0.0312), tensor(0.0233), 0.0, 0.0, tensor(0.0191), tensor(0.0236), tensor(0.0206), tensor(0.0117), 0.0, tensor(0.0473), tensor(0.0102), tensor(0.0193), 0.0, 0.0, 0.0, tensor(0.0495), 0.0, tensor(0.0422), tensor(0.0201), tensor(0.0421), 0.0, 0.0, 0.0, 0.0, tensor(0.0095), 0.0, tensor(0.0429), 0.0, 0.0, tensor(0.0387), 0.0, tensor(0.0335), tensor(0.0374), tensor(0.0252), tensor(0.0195), tensor(0.0311), 0.0, 0.0, 0.0, tensor(0.0337), tensor(0.0238), tensor(0.0299), 0.0, 0.0, 0.0, 0.0, tensor(0.0319), tensor(0.0384), tensor(0.0479), tensor(0.0260), tensor(0.0438), 0.0, 0.0, tensor(0.0481), 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, tensor(0.0333), tensor(0.0291), tensor(0.0405), 0.0, 0.0, 0.0, tensor(0.0324), tensor(0.0437), tensor(0.0084), tensor(0.0359), tensor(0.0274), 0.0, 0.0, 0.0, 0.0, tensor(0.0265), tensor(0.0169), tensor(0.0256), 0.0, 0.0, 0.0, 0.0, tensor(0.0376), tensor(0.0360), 0.0, 0.0, tensor(0.0149)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_iou(predicted_masks, gt_masks):\n",
        "    \"\"\"\n",
        "    Calculate IoU between predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        predicted_masks (list): List of predicted segmentation masks.\n",
        "        gt_masks (list): List of ground truth segmentation masks.\n",
        "    Returns:\n",
        "        float: Average IoU value.\n",
        "    \"\"\"\n",
        "    ious = []\n",
        "    for pred_mask, gt_mask in zip(predicted_masks, gt_masks):\n",
        "        iou = compute_iou(pred_mask, gt_mask)\n",
        "        ious.append(iou)\n",
        "\n",
        "    # Calculate average IoU\n",
        "    avg_iou = np.mean(ious)\n",
        "\n",
        "    return avg_iou\n",
        "\n",
        "def compute_iou(pred_mask, gt_mask):\n",
        "    \"\"\"\n",
        "    Compute IoU between predicted mask and ground truth mask.\n",
        "    Args:\n",
        "        pred_mask (numpy.ndarray): Predicted segmentation mask (binary).\n",
        "        gt_mask (numpy.ndarray): Ground truth segmentation mask (binary).\n",
        "    Returns:\n",
        "        float: IoU value.\n",
        "    \"\"\"\n",
        "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
        "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
        "    iou = intersection / union if union > 0 else 0.0\n",
        "    return iou\n",
        "\n",
        "# Example usage to calculate IoU for validation and test sets\n",
        "val_gt_masks = [cv2.imread(d[\"file_name\"])[:, :, 0] for d in val_dataset_dicts]  # Assuming ground truth masks are stored as images\n",
        "test_gt_masks = [cv2.imread(d[\"file_name\"])[:, :, 0] for d in test_dataset_dicts]  # Assuming ground truth masks are stored as images\n",
        "\n",
        "# Calculate average IoU for validation set\n",
        "avg_val_iou = calculate_iou(val_predictions, val_gt_masks)\n",
        "\n",
        "# Calculate average IoU for test set\n",
        "avg_test_iou = calculate_iou(test_predictions, test_gt_masks)\n",
        "\n",
        "# Print or process the average IoU values as needed\n",
        "print(f\"Average IoU for validation set: {avg_val_iou:.4f}\")\n",
        "print(f\"Average IoU for test set: {avg_test_iou:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vSq0pwDlyCo",
        "outputId": "6d54cc40-3464-4e72-9c36-144974823bf9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average IoU for validation set: 0.0157\n",
            "Average IoU for test set: 0.0143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boundary F1 Score for Validation and Test\n",
        "\n",
        "\n",
        "CAN'T FIGURE OUT -- code not working\n"
      ],
      "metadata": {
        "id": "l0AyLxfWm06i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get segmentation mask predictions for a dataset\n",
        "def get_segmentation_predictions(dataset_dicts, predictor):\n",
        "    predictions = []\n",
        "    for d in dataset_dicts:\n",
        "        im = cv2.imread(d[\"file_name\"])\n",
        "        outputs = predictor(im)\n",
        "        predictions.append(outputs[\"instances\"].to(\"cpu\").pred_masks)\n",
        "    return predictions\n",
        "\n",
        "# Get segmentation mask predictions for val and test datasets\n",
        "val_predictions = get_segmentation_predictions(val_dataset_dicts, predictor)\n",
        "test_predictions = get_segmentation_predictions(test_dataset_dicts, predictor)"
      ],
      "metadata": {
        "id": "e1Vl3sODnhe9"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from skimage.measure import find_contours\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def extract_boundary(mask):\n",
        "    \"\"\"\n",
        "    Extract boundary from a binary mask using find_contours.\n",
        "    Args:\n",
        "        mask (numpy.ndarray): Binary segmentation mask.\n",
        "    Returns:\n",
        "        numpy.ndarray: Binary mask of the boundary.\n",
        "    \"\"\"\n",
        "    contours = find_contours(mask, 0.5)\n",
        "    boundary = np.zeros_like(mask, dtype=np.uint8)\n",
        "\n",
        "    for contour in contours:\n",
        "        contour = np.round(contour).astype(np.int32)\n",
        "        boundary[contour[:, 0], contour[:, 1]] = 1\n",
        "\n",
        "    return boundary\n",
        "\n",
        "def calculate_boundary_f1(predicted_masks, gt_masks):\n",
        "    \"\"\"\n",
        "    Calculate Boundary F1 Score between predicted masks and ground truth masks.\n",
        "    Args:\n",
        "        predicted_masks (list): List of predicted segmentation masks.\n",
        "        gt_masks (list): List of ground truth segmentation masks.\n",
        "    Returns:\n",
        "        float: Boundary F1 Score.\n",
        "    \"\"\"\n",
        "    boundary_f1_scores = []\n",
        "\n",
        "    for pred_mask, gt_mask in zip(predicted_masks, gt_masks):\n",
        "        # Example check for empty masks (adjust as needed)\n",
        "        if np.count_nonzero(pred_mask) < 4 or np.count_nonzero(gt_mask) < 4:\n",
        "            continue  # Skip empty or very small masks\n",
        "\n",
        "        pred_boundary = extract_boundary(pred_mask)\n",
        "        gt_boundary = extract_boundary(gt_mask)\n",
        "\n",
        "        # Calculate F1 score based on boundaries\n",
        "        boundary_f1 = f1_score(gt_boundary.flatten(), pred_boundary.flatten())\n",
        "        boundary_f1_scores.append(boundary_f1)\n",
        "\n",
        "    # Calculate average Boundary F1 Score\n",
        "    avg_boundary_f1 = np.mean(boundary_f1_scores)\n",
        "\n",
        "    return avg_boundary_f1\n",
        "\n",
        "# Example usage to calculate Boundary F1 Score for validation and test sets\n",
        "val_predictions = get_segmentation_predictions(val_dataset_dicts, predictor)\n",
        "test_predictions = get_segmentation_predictions(test_dataset_dicts, predictor)\n",
        "\n",
        "val_gt_masks = [cv2.imread(d[\"file_name\"], cv2.IMREAD_GRAYSCALE) for d in val_dataset_dicts]\n",
        "test_gt_masks = [cv2.imread(d[\"file_name\"], cv2.IMREAD_GRAYSCALE) for d in test_dataset_dicts]\n",
        "\n",
        "# Calculate Boundary F1 Score for validation set\n",
        "avg_boundary_f1_val = calculate_boundary_f1(val_predictions, val_gt_masks)\n",
        "\n",
        "# Calculate Boundary F1 Score for test set\n",
        "avg_boundary_f1_test = calculate_boundary_f1(test_predictions, test_gt_masks)\n",
        "\n",
        "# Print or process the Boundary F1 Scores as needed\n",
        "print(\"Boundary F1 Score for validation set:\", avg_boundary_f1_val)\n",
        "print(\"Boundary F1 Score for test set:\", avg_boundary_f1_test)\n"
      ],
      "metadata": {
        "id": "5kSt2Ke5o3L8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
