{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORN7P1Dk6ZDSlNkAdU1/Dp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AvellinaLeong/NHM-Nannofossil-Segmentation-Project/blob/main/02_Standardise_Images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardise Image Size\n",
        "\n",
        "\n",
        "\n",
        "*   Cropped segmented instances all have varying sizes and shapes\n",
        "*   Therefore the largest one is found and black space is added to the rest, to standardise the size and shape of all cropped segmented instances\n",
        "\n"
      ],
      "metadata": {
        "id": "a5mmDASsolUO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgz4POEdnlkO",
        "outputId": "374bcd2a-78cf-4e8e-e24e-b5bbd34be348"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/development/avellina\n",
            "Binary_Classification_notebooks  Detectron2_notebooks  Morphometrics_notebooks\n",
            "detectron2\t\t\t Mask-RCNN\t       output\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "# Set script location to own development space\n",
        "MY_DEVELOPMENT_SPACE = '/content/drive/MyDrive/development/avellina/'\n",
        "import os\n",
        "os.chdir(MY_DEVELOPMENT_SPACE)\n",
        "!pwd\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "yRytNwOeo_5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to the directories\n",
        "train_dir = \"/content/drive/MyDrive/data/species_53/Binary_Classification/cropped_segmentations/train\"\n",
        "val_dir = \"/content/drive/MyDrive/data/species_53/Binary_Classification/cropped_segmentations/val\""
      ],
      "metadata": {
        "id": "mnheQzl4pBR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the Largest Cropped Image"
      ],
      "metadata": {
        "id": "QHmapJYno6VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the largest image\n",
        "def find_largest_image(directories):\n",
        "    max_width = 0\n",
        "    max_height = 0\n",
        "    max_size = 0\n",
        "    largest_image_path = \"\"\n",
        "\n",
        "    for directory in directories:\n",
        "        for root, _, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith(('png', 'jpg', 'jpeg')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with Image.open(file_path) as img:\n",
        "                        width, height = img.size\n",
        "                        size = width * height\n",
        "                        if size > max_size:\n",
        "                            max_width = width\n",
        "                            max_height = height\n",
        "                            max_size = size\n",
        "                            largest_image_path = file_path\n",
        "\n",
        "    return largest_image_path, max_width, max_height\n",
        "\n",
        "# Find the largest image in both directories\n",
        "largest_image_path, max_width, max_height = find_largest_image([train_dir, val_dir])\n",
        "\n",
        "# Print the path and dimensions of the largest image\n",
        "print(f\"Largest image: {largest_image_path}\")\n",
        "print(f\"Dimensions (width x height): {max_width} x {max_height}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_PsqtVro-F6",
        "outputId": "e4f13bf7-6dc0-4987-c6f7-c9a2acf2aab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest image: /content/drive/MyDrive/data/species_53/Binary_Classification/cropped_segmentations/train/PM_NF_5379_04_3_3.jpeg\n",
            "Dimensions (width x height): 206 x 155\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Largest image: /content/drive/MyDrive/data/species_53/Binary_Classification/cropped_segmentations/train/PM_NF_5379_04_3_3.jpeg\n",
        "# Dimensions (width x height): 206 x 155"
      ],
      "metadata": {
        "id": "HZX4-aBQAWSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pad all other images to scale to largest dimensions\n",
        "\n",
        "\n",
        "\n",
        "*   The largest dimensions is 206 x 155 pixels\n",
        "*   Pad the rest of the images with black space to match the largest sized image to standardise size across all cropped images\n",
        "\n"
      ],
      "metadata": {
        "id": "UGJHd8k3AU-w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Output directories for the padded images\n",
        "output_dir = \"/content/drive/MyDrive/data/species_53/Binary_Classification/standardised_cropped_segmentations\"\n",
        "train_output_dir = os.path.join(output_dir, \"train\")\n",
        "val_output_dir = os.path.join(output_dir, \"val\")\n",
        "\n",
        "# Create output directories if they don't exist\n",
        "os.makedirs(train_output_dir, exist_ok=True)\n",
        "os.makedirs(val_output_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "zn07xngtCrwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find the largest image\n",
        "def find_largest_image(directories):\n",
        "    max_width = 0\n",
        "    max_height = 0\n",
        "    max_size = 0\n",
        "    largest_image_path = \"\"\n",
        "\n",
        "    for directory in directories:\n",
        "        for root, _, files in os.walk(directory):\n",
        "            for file in files:\n",
        "                if file.endswith(('png', 'jpg', 'jpeg')):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with Image.open(file_path) as img:\n",
        "                        width, height = img.size\n",
        "                        size = width * height\n",
        "                        if size > max_size:\n",
        "                            max_width = width\n",
        "                            max_height = height\n",
        "                            max_size = size\n",
        "                            largest_image_path = file_path\n",
        "\n",
        "    return largest_image_path, max_width, max_height"
      ],
      "metadata": {
        "id": "fpaT7sD1Cxa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad images to make them square and of the same size\n",
        "def pad_images_to_square(input_directories, output_directories, target_size):\n",
        "    for input_directory, output_directory in zip(input_directories, output_directories):\n",
        "        for root, _, files in os.walk(input_directory):\n",
        "            for file in files:\n",
        "                if file.endswith(('png', 'jpg', 'jpeg')):\n",
        "                    input_file_path = os.path.join(root, file)\n",
        "                    with Image.open(input_file_path) as img:\n",
        "                        width, height = img.size\n",
        "                        new_image = Image.new(\"RGB\", (target_size, target_size), (0, 0, 0))  # Create a black background\n",
        "                        new_image.paste(img, ((target_size - width) // 2, (target_size - height) // 2))  # Center the original image\n",
        "\n",
        "                        # Construct the output file path\n",
        "                        relative_path = os.path.relpath(input_file_path, input_directory)\n",
        "                        output_file_path = os.path.join(output_directory, relative_path)\n",
        "\n",
        "                        # Create the output directory if it doesn't exist\n",
        "                        os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "                        # Save the new image\n",
        "                        new_image.save(output_file_path)\n",
        "\n",
        "# Find the largest image in both directories\n",
        "largest_image_path, max_width, max_height = find_largest_image([train_dir, val_dir])\n",
        "\n",
        "# Determine the target size for all images (making them square)\n",
        "target_size = max(max_width, max_height)\n",
        "\n",
        "# Pad all images to the target size and save them to the output directories\n",
        "pad_images_to_square([train_dir, val_dir], [train_output_dir, val_output_dir], target_size)\n",
        "\n",
        "print(f\"All images have been padded to {target_size}x{target_size} and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2rApnamCz3J",
        "outputId": "d652fa59-3629-42db-d927-b2692278f5bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All images have been padded to 206x206 and saved successfully.\n"
          ]
        }
      ]
    }
  ]
}